
# Criar **Job ETL** no AWS Glue (de `raw` â†’ `gold` em Parquet)

Este guia cria um **Job do AWS Glue** (Spark/Python) que lÃª dados **JSON** catalogados na camada **raw**, **achata** a estrutura e grava em **Parquet** particionado por **ano/mÃªs/dia** na camada **gold**.

---

## âœ… Objetivo
- Ler tabelas do **Glue Data Catalog** (criadas pelo *crawler* da camada **raw**).
- **Flatten** (achatamento) do JSON para colunas tabulares.
- Gravar em **S3** em `s3://<seu-bucket>/gold/<YYYY>/<MM>/<DD>/` no formato **Parquet**.

---

## ðŸ“‹ PrÃ©â€‘requisitos
- **Crawler** da camada **raw** executado e **database** (ex.: `raw_db`) criado no **Data Catalog**.
- **Bucket S3** com a pasta **`gold/`**.
- **IAM Role** para Glue (ex.: `ETL-Role`) com:
  - `AWSGlueServiceRole`
  - `AmazonS3FullAccess`
  - `CloudWatchLogsFullAccess`

> Em produÃ§Ã£o, substitua *FullAccess* por permissÃµes de **menor privilÃ©gio**.

---

## 1) Abrir o Glue Studio e criar o Job
1. No Console da AWS, procure **AWS Glue**.
2. Acesse **ETL Jobs** (Glue Studio).
3. Clique em **Create** â†’ escolha **Script editor** ( Author with a script editor ).
4. **Type**: `Spark` (linguagem `Python`).
5. Selecione **Start fresh** para comeÃ§ar com script em branco.

---

## 2) Preparar o script
1. Cole o **script ETL** do curso no editor.
2. **Atualize os parÃ¢metros** no topo do script (conforme o cÃ³digo do curso):
   - **Database** do catÃ¡logo (ex.: `raw_db`).
   - **OUTPUT_PATH**: S3 da camada **gold**.  
     - No S3, abra **`gold/`** â†’ **Copy S3 URI** e cole no parÃ¢metro (ex.: `s3://meu-bucket/gold/`).
3. O script deve:
   - Inicializar **Spark/GlueContext**.
   - Listar **tabelas** do `raw_db`.
   - Ler os **JSON** (DataSource do catÃ¡logo).
   - **Flatten** (transformar `struct`/JSON em colunas).
   - Extrair **ano/mÃªs/dia** para particionamento.
   - Gravar em **Parquet** no **OUTPUT_PATH** com partiÃ§Ãµes `year/month/day`.

> ObservaÃ§Ã£o: O *flatten* Ã© a parte que transforma os campos aninhados (`struct`) em colunas tabulares.

---

## 3) Definir detalhes do Job
1. Em **Job details**:
   - **Name**: `weather-job` (ou outro nome do seu padrÃ£o).
   - **IAM role**: selecione **`ETL-Role`** (criada anteriormente).
2. (Opcional) Ajuste recursos (workers, Glue version) conforme sua conta e limites.

Clique em **Save** para salvar o script e as configuraÃ§Ãµes.

---

## 4) Executar e acompanhar
1. Clique em **Run** para iniciar o Job.
2. Em **Runs**, acompanhe o status ( `Running` â†’ `Succeeded` ).
3. Consulte **CloudWatch Logs** em caso de erros.

---

## 5) Validar saÃ­da no S3
1. Abra **Amazon S3** â†’ seu bucket â†’ **`gold/`**.
2. Verifique a criaÃ§Ã£o de pastas **`YYYY/MM/DD/`**.
3. Confirme a presenÃ§a de arquivos **`.parquet`**.
4. Baixe um Parquet (opcional) e valide o schema com alguma ferramenta/local.

---

## 6) Agendamento e incremental (opcionais)
- **Schedule no Glue**: defina uma periodicidade (ex.: 1x/dia ou a cada 6h) para rodar o Job em *batch*.
- **Job bookmark**: habilite para que o Glue **nÃ£o reprocese** dados jÃ¡ carregados.
  - Requer um campo de referÃªncia (geralmente **data/hora**) no script.

---

## âœ… Resultado
O **Job ETL do Glue** lÃª a camada **raw**, achata o JSON e grava a camada **gold** em **Parquet** particionado por data, pronto para consultas (ex.: via **Athena**, Redshift, etc.).

> **PrÃ³ximo passo**: rode um **crawler na pasta `gold/`** para catalogar as tabelas e consultar com o **Athena**.
